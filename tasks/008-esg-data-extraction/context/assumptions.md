# Assumptions Document
## Task 008: ESG Maturity Evidence Extraction

**Task ID:** 008-esg-data-extraction
**Date:** 2025-10-22
**Protocol:** SCA v13.8-MEA
**Version:** 1.0

---

## Purpose

This document explicitly states all assumptions underlying the evidence extraction methodology for Task 008. Each assumption includes:
1. **Statement:** The assumption being made
2. **Rationale:** Why this assumption is necessary
3. **Risk if Invalid:** What happens if assumption is wrong
4. **Validation Method:** How we verify the assumption holds

Assumptions are categorized by subsystem: Data Sources, Pattern Matching, Evidence Extraction, Stage Classification, and Performance.

---

## Data Source Assumptions

### A1: SEC Filing Structure Consistency

**Statement:** SEC 10-K/20-F filings contain ESG disclosures in Item 1 (Business), Item 1A (Risk Factors), Item 7 (MD&A), or dedicated sustainability sections with reasonable consistency across companies.

**Rationale:** Pattern matching relies on ESG content being present in filings, regardless of exact location. We assume ESG disclosure is not relegated to external sustainability reports alone.

**Risk if Invalid:**
- Theme coverage falls below 4/7 threshold
- Extraction rate drops below 60% (15/25 reports)
- Heavy bias toward companies with dedicated ESG sections

**Validation Method:**
- Manual inspection of 5 filings to verify ESG content presence
- Measure theme coverage distribution across 25 filings
- Document section locations where evidence is found

**Likelihood:** High (SEC climate disclosure rules encourage 10-K inclusion)

---

### A2: HTML Parsing Robustness

**Statement:** BeautifulSoup4 can reliably extract text from SEC EDGAR HTML files despite inconsistent HTML structure, nested tables, and styling variations.

**Rationale:** SEC filings are generated by different companies using different XBRL/HTML conversion tools, leading to structural variability. BeautifulSoup must handle malformed HTML gracefully.

**Risk if Invalid:**
- Parsing failures causing incomplete text extraction
- Text extraction loses paragraph boundaries (affects context windows)
- Page number estimation is inaccurate

**Validation Method:**
- Test parsing on all 25 downloaded filings
- Log any BeautifulSoup parsing warnings
- Manually inspect 5 extracted text samples for completeness

**Likelihood:** High (BeautifulSoup is designed for malformed HTML)

---

### A3: Text Encoding Consistency

**Statement:** SEC HTML files use UTF-8 or ASCII encoding, and special characters (em dashes, smart quotes, degree symbols) do not break pattern matching.

**Rationale:** Regex patterns may not account for Unicode variants (e.g., "–" vs "-", """ vs "\"", "°C" vs "degrees C").

**Risk if Invalid:**
- Pattern matching misses evidence due to character encoding mismatches
- Extracted 30-word context contains garbled characters

**Validation Method:**
- Normalize Unicode characters during text extraction (NFKC normalization)
- Test patterns against sample text with special characters
- Log any encoding errors during parsing

**Likelihood:** Medium (SEC filings generally use standard encoding, but special chars exist)

---

## Pattern Matching Assumptions

### A4: Keyword Proximity Indicates Relevance

**Statement:** When target keywords (e.g., "SBTi", "Scope 1", "TCFD") appear within 30 words of contextual keywords (e.g., "committed", "target", "emissions"), the match is likely relevant evidence.

**Rationale:** Contextual validation reduces false positives compared to standalone keyword matching. 30-word windows capture 1-2 sentences of context.

**Risk if Invalid:**
- False positives: Keywords appear in unrelated contexts (e.g., "risk" in financial risk sections, not climate risk)
- False negatives: Evidence is paraphrased beyond keyword matching (e.g., "1.5-degree pathway" instead of "SBTi")

**Validation Method:**
- Manual validation of 50 evidence samples (5 filings × 10 evidence items)
- Measure precision (% of extracted evidence that is relevant)
- Target: ≥75% precision

**Likelihood:** Medium-High (contextual keywords improve precision, but paraphrasing still causes misses)

---

### A5: Regex Patterns Generalize Across Companies

**Statement:** The regex patterns designed for 7 themes generalize across the 25 companies in the sample (15 US, 4 Europe, 6 Asia-Pacific).

**Rationale:** Companies use similar terminology when referencing standard frameworks (GRI, TCFD, SBTi, GHG Protocol) due to regulatory convergence and industry best practices.

**Risk if Invalid:**
- Low recall for non-US companies (20-F filings may use different terminology)
- Low recall for companies in certain industries (e.g., financial services vs. manufacturing)
- Bias toward companies using English-language frameworks

**Validation Method:**
- Analyze evidence distribution by region (US vs. Europe vs. Asia-Pacific)
- Analyze evidence distribution by industry
- If recall is <60% for any region, expand patterns

**Likelihood:** Medium (frameworks are globalizing, but terminology varies)

---

### A6: Confidence Scoring Formula is Calibrated

**Statement:** The confidence scoring formula (keyword match strength + context validation + negation check) produces scores that correlate with actual evidence relevance.

**Rationale:** Confidence scores guide Stage Classifier and enable filtering low-quality evidence. Formula is based on heuristic rules, not trained model.

**Risk if Invalid:**
- Confidence scores do not distinguish high-quality vs. low-quality evidence
- Manual validation shows weak correlation between confidence and precision
- Stage Classifier over-relies on low-confidence evidence

**Validation Method:**
- Manual validation labels 50 evidence items as "relevant" or "not relevant"
- Calculate precision@0.5, precision@0.7, precision@0.9 thresholds
- Adjust formula if precision@0.5 < 75%

**Likelihood:** Medium (heuristic confidence scoring is unvalidated)

---

## Evidence Extraction Assumptions

### A7: 30-Word Context is Sufficient

**Statement:** A 30-word window (15 words before + match + 15 words after) provides sufficient context for human reviewers and Stage Classifier to determine evidence relevance.

**Rationale:** ESG Maturity Rubric specifies 30-word extracts (line 119). 30 words typically span 1-2 sentences, enough to understand claim without full paragraph.

**Risk if Invalid:**
- Context windows truncate mid-sentence, losing meaning
- Reviewers cannot determine relevance without broader context
- Evidence classification requires access to full document sections

**Validation Method:**
- Manual review of 5 filings (50 evidence items)
- Reviewers rate: "Can you determine relevance from this 30-word extract?"
- If <80% of evidence is interpretable, increase window size

**Likelihood:** Medium-High (30 words is standard, but some evidence may need more context)

---

### A8: SHA256 Deduplication is Effective

**Statement:** SHA256 hashing of extract_30w effectively deduplicates evidence without losing unique evidence items.

**Rationale:** The same evidence statement may appear multiple times in a filing (executive summary, MD&A, footnotes). Exact text deduplication avoids inflating evidence counts.

**Risk if Invalid:**
- Different 30-word windows around the same core evidence are NOT deduplicated (e.g., slightly different surrounding sentences)
- Exact deduplication is too strict, preventing identification of multiple instances of similar evidence

**Validation Method:**
- Measure deduplication rate (% of evidence removed as duplicates)
- Expected: 10-30% deduplication rate
- Manual inspection of 10 deduplicated pairs to verify they are true duplicates

**Likelihood:** High (exact matching is deterministic, but may miss near-duplicates)

---

### A9: Page Number Estimation is Acceptable

**Statement:** Approximating page numbers from HTML structure (e.g., dividing character position by avg. characters per page) is acceptable for traceability, even if not perfectly accurate.

**Rationale:** HTML does not have inherent page numbers. Approximate page numbers enable users to find evidence in original filings.

**Risk if Invalid:**
- Page numbers are off by ±5 pages, making evidence difficult to locate
- Users lose trust in traceability

**Validation Method:**
- Manual check of 10 evidence items: Does stated page_no correspond to location in original filing?
- Acceptable error: ±3 pages

**Likelihood:** Medium (HTML-to-page mapping is approximate, but acceptable for rough navigation)

---

## Stage Classification Assumptions

### A10: Conservative Scoring Reduces False Positives

**Statement:** When evidence is ambiguous or weak, assigning a lower stage (e.g., Stage 2 instead of Stage 3) reduces false positives and increases credibility.

**Rationale:** Over-scoring maturity is worse than under-scoring, as it inflates company capabilities and reduces trust in the assessment.

**Risk if Invalid:**
- Companies are systematically under-scored, even when they have strong ESG practices
- Bias toward Stage 1-2 even when Stage 3-4 evidence exists
- Manual validation reveals conservative logic is too strict

**Validation Method:**
- Manual validation of 5 filings: Compare assigned stages to expert judgment
- If >20% of cases are under-scored by ≥2 stages, relax conservative thresholds

**Likelihood:** Medium (conservative logic is defensible, but may need tuning)

---

### A11: Multiple Evidence Items Indicate Higher Stages

**Statement:** Requiring multiple pieces of Stage 4 evidence (e.g., ≥2 items) to assign Stage 4 prevents false upgrades from single weak matches.

**Rationale:** Stage 4 represents "Leading" maturity, requiring strong, corroborated evidence (e.g., SBTi validated + reasonable assurance + financial integration).

**Risk if Invalid:**
- Companies with single strong Stage 4 evidence item (but no corroborating evidence) are downgraded to Stage 3
- Multi-evidence threshold is too strict for themes with sparse disclosure (DM, EI)

**Validation Method:**
- Analyze Stage 4 assignment distribution across 25 filings
- If <10% of companies reach Stage 4 in any theme, relax multi-evidence threshold

**Likelihood:** Medium-High (multi-evidence threshold is conservative but reasonable)

---

### A12: Evidence Type Mapping is Accurate

**Statement:** Each evidence type (e.g., "SBTi_validated", "limited_assurance", "scenario_analysis") maps to a specific stage indicator (0-4) as defined in design.md.

**Rationale:** Stage assignment logic relies on evidence type classifications. Mappings are based on ESG Maturity Rubric descriptors.

**Risk if Invalid:**
- Evidence types are misclassified (e.g., "SBTi committed" assigned Stage 4 instead of Stage 3)
- Stage assignment logic does not align with rubric intent

**Validation Method:**
- Manual review of evidence type → stage mappings against rubric
- Expert validation of 5 filings' stage assignments

**Likelihood:** Medium-High (mappings are based on rubric, but interpretation may vary)

---

## Performance Assumptions

### A13: Parallel Processing Achieves 4× Speedup

**Statement:** Using 4 worker processes achieves ~4× speedup compared to sequential processing, reducing total batch time from ~750s to ~190s for 25 filings.

**Rationale:** Pattern matching is CPU-bound with minimal I/O contention. Linear speedup is typical for embarrassingly parallel workloads.

**Risk if Invalid:**
- Memory contention reduces parallelism efficiency
- Python GIL (Global Interpreter Lock) limits CPU parallelism
- Speedup is only 2-3× instead of 4×

**Validation Method:**
- Measure actual batch processing time for 25 filings
- Compare sequential vs. 4-worker parallel execution
- Acceptable: ≥3× speedup (≥225s total time)

**Likelihood:** High (multiprocessing bypasses GIL, pattern matching is CPU-intensive)

---

### A14: Processing Time Target is Achievable

**Statement:** P95 processing time per filing is <45 seconds, meeting performance requirements for interactive use.

**Rationale:** Breakdown: HTML parsing (~5-10s) + pattern matching (~10-20s) + evidence aggregation (~2-5s) = ~17-35s typical, <45s P95.

**Risk if Invalid:**
- Large filings (>200 pages) exceed 45s processing time
- P95 processing time is 60-90s, acceptable for batch but not interactive use

**Validation Method:**
- Measure processing time distribution for all 25 filings
- Report P50, P95, P99 processing times
- Threshold: P95 <90s

**Likelihood:** High (regex is fast, batch processing is acceptable for PoC)

---

### A15: JSON Storage is Sufficient for 25 Filings

**Statement:** Storing evidence in JSON files (one per filing) is sufficient for 25 filings, with migration to database in Task 010.

**Rationale:** 25 filings × 10-20 evidence items = ~250-500 evidence records. JSON is human-readable and easy to inspect.

**Risk if Invalid:**
- JSON files become too large (>10 MB each) due to high evidence counts
- Aggregation queries require loading all JSON files into memory
- JSON parsing is slow compared to SQL queries

**Validation Method:**
- Measure total JSON file size for all evidence
- Acceptable: <50 MB total, <5 MB per file
- If total size >100 MB, migrate to SQLite immediately

**Likelihood:** High (25 filings is small, JSON is appropriate for PoC)

---

## Quality Validation Assumptions

### A16: 5 Manual Reviews are Sufficient for Precision Measurement

**Statement:** Manually validating 5 filings (50 evidence items) provides a representative sample to measure extraction precision (≥75% target).

**Rationale:** 5 filings cover ~20% of the dataset. Stratified sampling (by region/industry) ensures diversity.

**Risk if Invalid:**
- 5 filings are not representative of the 25-filing dataset
- Precision varies significantly by theme, region, or industry
- Sample size is too small for statistical significance (wide confidence intervals)

**Validation Method:**
- Stratified sampling: 2 US, 1 Europe, 2 Asia-Pacific filings
- Measure precision separately by theme
- If precision variance >20% across themes, expand sample to 10 filings

**Likelihood:** Medium (5 filings is a reasonable PoC sample, but not statistically rigorous)

---

### A17: Precision ≥75% is Acceptable for PoC

**Statement:** Achieving ≥75% precision (≥75% of extracted evidence is relevant) is sufficient for Task 008 PoC, with higher precision targeted in Task 009 ML enhancements.

**Rationale:** 75% precision means 1 in 4 evidence items is a false positive. Acceptable for PoC where manual validation is used, but not for production.

**Risk if Invalid:**
- False positive rate (25%) is too high for stakeholder trust
- Stage Classifier is misled by irrelevant evidence
- Manual validation workload is excessive

**Validation Method:**
- Measure actual precision on 5 manually validated filings
- If precision <75%, refine patterns and re-run extraction
- If precision >85%, document success

**Likelihood:** Medium (75% is achievable with contextual validation, but untested)

---

## Data Lineage Assumptions

### A18: Task 007 Downloads are Complete and Valid

**Statement:** All 25 SEC filings downloaded in Task 007 are complete (no truncation), valid HTML, and correspond to the most recent 10-K/20-F for each company.

**Rationale:** Evidence extraction depends on high-quality input data. Corrupted or outdated filings would invalidate results.

**Risk if Invalid:**
- Parsing failures due to incomplete HTML
- Outdated filings do not reflect current ESG maturity
- SHA256 hashes do not match expected values

**Validation Method:**
- Verify file sizes are reasonable (500 KB - 5 MB)
- Check SHA256 hashes against manifest (tasks/008-esg-data-extraction/data/filing_hashes.json)
- Verify filing dates are 2024-2025

**Likelihood:** High (Task 007 reported 100% download success)

---

### A19: ESG Maturity Rubric v3.0 is Stable

**Statement:** The ESG Maturity Rubric v3.0 (7 themes, stage descriptors) is stable and will not change during Task 008 implementation.

**Rationale:** Evidence extraction patterns and stage mappings are tightly coupled to rubric definitions. Changes to rubric would require pattern updates.

**Risk if Invalid:**
- Rubric is updated mid-task, requiring pattern refactoring
- Stage definitions change, invalidating stage assignments

**Validation Method:**
- Lock rubric version at v3.0 for Task 008
- Document rubric version in all evidence output
- If rubric changes, treat as new task (Task 008b)

**Likelihood:** High (rubric is versioned and stable)

---

## Exclusions and Scope Assumptions

### A20: Out-of-Scope Metrics Do Not Affect Results

**Statement:** Water usage, waste/recycling, employee diversity, safety metrics, board composition, and supply chain labor practices are OUT OF SCOPE and will not be extracted, even if present in filings.

**Rationale:** ESG Maturity Rubric v3.0 focuses on climate/sustainability maturity (7 themes: TSP, OSP, DM, GHG, RD, EI, RMM), not broad E/S/G metrics.

**Risk if Invalid:**
- Users expect broad ESG data extraction and are disappointed by narrow scope
- Evidence extraction misses relevant ESG maturity indicators in social/governance domains

**Validation Method:**
- Explicitly document exclusions in hypothesis.md and design.md
- Communicate scope limitations to stakeholders
- If stakeholders request expanded scope, treat as Task 009b enhancement

**Likelihood:** High (scope is explicitly defined in rubric)

---

## Summary Table

| ID | Assumption | Category | Likelihood | Validation Method |
|----|-----------|----------|------------|-------------------|
| A1 | SEC Filing Structure Consistency | Data Sources | High | Manual inspection of 5 filings |
| A2 | HTML Parsing Robustness | Data Sources | High | Test parsing on all 25 filings |
| A3 | Text Encoding Consistency | Data Sources | Medium | Unicode normalization + testing |
| A4 | Keyword Proximity Indicates Relevance | Pattern Matching | Medium-High | Manual validation of 50 samples |
| A5 | Regex Patterns Generalize Across Companies | Pattern Matching | Medium | Analyze evidence by region/industry |
| A6 | Confidence Scoring Formula is Calibrated | Pattern Matching | Medium | Precision@0.5/0.7/0.9 analysis |
| A7 | 30-Word Context is Sufficient | Evidence Extraction | Medium-High | Manual review of 50 evidence items |
| A8 | SHA256 Deduplication is Effective | Evidence Extraction | High | Measure deduplication rate (10-30%) |
| A9 | Page Number Estimation is Acceptable | Evidence Extraction | Medium | Manual check of 10 evidence items |
| A10 | Conservative Scoring Reduces False Positives | Stage Classification | Medium | Expert validation of 5 filings |
| A11 | Multiple Evidence Items Indicate Higher Stages | Stage Classification | Medium-High | Analyze Stage 4 distribution |
| A12 | Evidence Type Mapping is Accurate | Stage Classification | Medium-High | Manual review of mappings |
| A13 | Parallel Processing Achieves 4× Speedup | Performance | High | Measure sequential vs. parallel time |
| A14 | Processing Time Target is Achievable | Performance | High | Measure P50/P95/P99 processing times |
| A15 | JSON Storage is Sufficient for 25 Filings | Performance | High | Measure total JSON size (<50 MB) |
| A16 | 5 Manual Reviews are Sufficient | Quality Validation | Medium | Stratified sampling + precision measurement |
| A17 | Precision ≥75% is Acceptable for PoC | Quality Validation | Medium | Measure actual precision |
| A18 | Task 007 Downloads are Complete and Valid | Data Lineage | High | Verify SHA256 hashes + file sizes |
| A19 | ESG Maturity Rubric v3.0 is Stable | Data Lineage | High | Lock rubric version at v3.0 |
| A20 | Out-of-Scope Metrics Do Not Affect Results | Exclusions | High | Document scope limitations |

---

## Assumption Validation Plan

**Phase 1: Pre-Implementation (Current)**
- [x] Document all assumptions in assumptions.md
- [ ] Review assumptions with stakeholders
- [ ] Identify high-risk assumptions (likelihood <Medium) for early validation

**Phase 2: Implementation**
- [ ] Implement pattern matchers with assumption logging (e.g., encoding errors, parsing failures)
- [ ] Run extraction on 3 pilot filings to test A1-A9
- [ ] Measure preliminary precision, deduplication rate, processing time

**Phase 3: Validation**
- [ ] Run full extraction on 25 filings
- [ ] Manual validation of 5 filings (A4, A7, A10, A16, A17)
- [ ] Analyze evidence distribution by theme/region/industry (A5)
- [ ] Measure performance metrics (A13, A14, A15)
- [ ] Generate validation report documenting which assumptions held

**Phase 4: Refinement**
- [ ] If any assumption is invalidated, document remediation in ADR
- [ ] Adjust patterns, confidence formula, or stage logic as needed
- [ ] Re-run extraction if changes affect >10% of evidence

---

**Version:** 1.0
**Status:** All assumptions documented
**Next:** Create cp_paths.json, then proceed to implementation
