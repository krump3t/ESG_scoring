code,severity,path,line,excerpt,why,proposed_fix
AV01,high,mcp_report_fetcher.py,211,"base_score = random.uniform(0.3, 0.9)","Quality assessment seeds nothing, so MCP scores change every run.","Inject deterministic seed (e.g., company/year hash) or remove the random simulation entirely."
AV01,medium,libs/storage/astradb_vector.py,489,test_embedding = np.random.randn(self.config.embedding_dimension),"Health check inserts random vectors without seeding, making store state nondeterministic.",Use a deterministic vector (zeros or seeded RNG) for connectivity checks.
AV02,high,pipelines/airflow/dags/esg_scoring_dag.py,260,"chunk_id = chunk_data.get(""chunk_id"", f""chunk_{hash(chunk_data['text'])}"")",Python hash() changes per process so chunk IDs and downstream trace IDs mutate run-to-run.,Replace hash() with a stable digest such as hashlib.sha256(text.encode()).hexdigest().
AV02,medium,libs/query/query_synthesizer.py,347,template_idx = (hash(company_id + user_query) % len(templates)),"Template selection uses hash(), so identical queries can hit different templates depending on PYTHONHASHSEED.",Derive template_idx from a sha256 digest or deterministic PRNG seeded by company/query.
AV03,high,apps/pipeline/demo_flow.py,361,"""timestamp"": time.time()","Parity artifact embeds wall-clock seconds, preventing reproducible bytes-on-disk.",Record an audit timestamp derived from deterministic inputs or a provided clock override.
AV03,medium,apps/evaluation/response_quality.py,132,"response_id = hashlib.md5(f""{query}{response}{datetime.now()}"".encode())","Response IDs mix in datetime.now(), so identical inputs generate different IDs and traces.",Build response_id from deterministic inputs (company/query hash or AUDIT_TIME env) only.
AV04,critical,apps/scoring/pipeline.py,95,self.llm_client = get_watsonx_client(),"Production ESGScoringPipeline always boots live WatsonX/Astra clients, so scoring requires network egress.",Provide an offline scoring path (deterministic embedder/vector store) and gate network clients behind optional flags disabled in CP.
AV05,high,apps/ingestion/parser.py,94,"response = requests.get(url, timeout=30)","PDF downloads happen directly via requests with no IngestLedger entry, so provenance loses URL/headers/sha256.","Plumb every fetch through IngestLedger (capture headers, SHA256, content_type) and enforce allow-listed hosts."
AV05,high,apps/ingestion/report_fetcher.py,207,"response = requests.get(pdf_url, timeout=30)",Report fetcher also pulls arbitrary URLs without ledger logging or allow-list checks.,"Route downloads through a ledger-aware helper that records URL, headers, hash, and blocks non-approved domains."
AV06,critical,artifacts/pipeline_validation/topk_vs_evidence.json,1,"""top5_doc_ids"": [""doc_1"", ...], ""evidence_doc_ids"": [""LSE_HEAD_2025_p4"", ...]","None of the recorded evidence doc_ids appear in the fused top-k lists (21/21 missing), so parity proof is invalid.",Regenerate the parity artifact from the actual fused results (real chunk IDs) and fail the gate until evidence ? top-k.
AV07,high,apps/pipeline/demo_flow.py,310,scorer = RubricV3Scorer(),"API scoring relies on heuristic code, not rubrics/maturity_v3.json, and does not enforce the =2 quotes per theme rule.","Switch to agents.scoring.RubricScorer (loads canonical JSON, MIN_QUOTES_PER_THEME) so runtime matches rubric spec."
AV08,medium,agents/crawler/data_providers/sasb_provider.py,183,"except Exception:
    return []","Provider errors are swallowed silently, resulting in empty data with no alert or provenance entry.",Log the exception and raise a ProviderError so ingestion fails fast and is visible in the ledger/CI.
AV09,medium,apps/ingestion/parser.py,65,"self.cache_dir = cache_dir or Path(""data/pdf_cache"")","Parser caches outside artifacts/, so ingestion can write anywhere on the filesystem.",Relocate cache_dir under artifacts/ (or make it configurable) so runs stay sandboxed.
AV09,medium,apps/evaluation/response_quality.py,745,"report_dir = Path(""reports/quality_evaluation"")","Response quality reports are persisted under reports/, not artifacts/, breaking workspace determinism.","Emit quality reports under artifacts/ (e.g., artifacts/reports/quality) or honor an injected output root."
AV11,medium,scripts/embed_and_index.py,111,data = json.loads(bronze_path.read_text())  # file ends with .parquet,"The embedding script reads JSON text from files labeled .parquet, so downstream jobs cannot trust the file format.",Write actual Parquet tables (pandas/pyarrow) or rename the stub files to .json so metadata matches reality.
