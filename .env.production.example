# ESG Evaluation Platform - Production Environment Template
#
# Copy this file to .env.production and fill in your actual values.
# NEVER commit .env.production to version control (it's git-ignored).
# Rotate API keys every 90 days.

# ============================================================================
# IBM watsonx.ai Configuration
# ============================================================================
# Get credentials from: https://cloud.ibm.com/iam/apikeys
# Copy your IBM Cloud API key here
IBM_WATSONX_API_KEY=your-watsonx-api-key-here

# Get project ID from: https://dataplatform.cloud.ibm.com
IBM_WATSONX_PROJECT_ID=your-project-id-here

# Regional endpoint (us-south, eu-de, jp-tok, etc.)
IBM_WATSONX_URL=https://us-south.ml.cloud.ibm.com

# ============================================================================
# AstraDB Configuration
# ============================================================================
# Get token from: https://astra.datastax.com
# Click "Generate Token" â†’ Select "Org Admin" role
ASTRA_DB_APPLICATION_TOKEN=your-astradb-token-here

# Database ID visible in AstraDB console URL
# Example URL: https://db-xxx-yyy-zzz-region.apps.astra.datastax.com
ASTRA_DB_ID=your-database-id-here

# Database region (us-east1, eu-west-1, etc.)
ASTRA_DB_REGION=us-east1

# Keyspace for vector storage (created in AstraDB)
ASTRA_DB_KEYSPACE=esg_vectors

# ============================================================================
# ngrok Configuration
# ============================================================================
# Get auth token from: https://dashboard.ngrok.com/auth/your-authtoken
NGROK_AUTH_TOKEN=your-ngrok-auth-token-here

# ============================================================================
# MCP Server Configuration
# ============================================================================
# Generate via: python -c "import secrets; print(secrets.token_hex(16))"
MCP_API_KEY=your-generated-api-key-here

# Rate limiting (requests per hour per IP)
MCP_RATE_LIMIT=100

# ============================================================================
# DuckDB Configuration
# ============================================================================
# Local SQLite-like database file path
DUCKDB_PATH=data_lake/analytics.duckdb

# Parquet data lake base directory
PARQUET_BASE_PATH=data_lake/parquet/

# ============================================================================
# Redis Configuration
# ============================================================================
# Redis host (from docker-compose: service name "redis")
REDIS_HOST=redis

# Redis port (default 6379)
REDIS_PORT=6379

# ============================================================================
# Airflow Configuration
# ============================================================================
# Generate Fernet key via:
# python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
AIRFLOW_FERNET_KEY=your-fernet-key-here

# Airflow executor (LocalExecutor for single-node, CeleryExecutor for distributed)
AIRFLOW__CORE__EXECUTOR=LocalExecutor

# PostgreSQL connection string for Airflow metadata
AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow

# ============================================================================
# Logging Configuration
# ============================================================================
# Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO

# Log file directory
LOG_DIR=logs/

# ============================================================================
# Security Notes
# ============================================================================
# 1. NEVER commit this file to git (use .gitignore)
# 2. Rotate API keys every 90 days
# 3. Use strong, random values for generated secrets
# 4. For IBM Code Engine: migrate to Secrets Manager (not .env files)
# 5. Audit who has access to this file locally
