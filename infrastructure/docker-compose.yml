version: '3.8'

# ============================================================================
# ESG Evaluation Platform - Production Infrastructure
# Phase 1: Local Docker Development Environment (9 Services)
# ============================================================================

services:

  # =========================================================================
  # DATABASE: PostgreSQL (Airflow Metadata)
  # =========================================================================
  postgres:
    image: postgres:15-alpine
    container_name: postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - esg_network
    restart: unless-stopped


  # =========================================================================
  # CACHE: Redis
  # =========================================================================
  redis:
    image: redis:7-alpine
    container_name: redis
    command: redis-server --appendonly yes
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    networks:
      - esg_network
    restart: unless-stopped


  # =========================================================================
  # OBJECT STORAGE: MinIO (S3-Compatible)
  # =========================================================================
  minio:
    image: minio/minio:RELEASE.2024-10-13T13-34-11Z
    container_name: minio
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - esg_network
    restart: unless-stopped


  # =========================================================================
  # CATALOG: Iceberg REST Catalog
  # =========================================================================
  iceberg-rest:
    image: tabulario/iceberg-rest:1.5.0
    container_name: iceberg-rest
    environment:
      CATALOG_WAREHOUSE: /warehouse
      CATALOG_IO_IMPL: org.apache.iceberg.aws.s3.S3FileIO
      CATALOG_S3_ENDPOINT: http://minio:9000
      CATALOG_S3_ACCESS_KEY_ID: minioadmin
      CATALOG_S3_SECRET_ACCESS_KEY: minioadmin
    ports:
      - "8181:8181"
    volumes:
      - iceberg_warehouse:/warehouse
    depends_on:
      minio:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8181/v1/config"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - esg_network
    restart: unless-stopped


  # =========================================================================
  # QUERY ENGINE: Trino
  # =========================================================================
  trino:
    image: trinodb/trino:438
    container_name: trino
    ports:
      - "8082:8080"
    environment:
      TRINO_ENVIRONMENT: production
    depends_on:
      minio:
        condition: service_healthy
      iceberg-rest:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/v1/info"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - esg_network
    restart: unless-stopped


  # =========================================================================
  # ORCHESTRATION: Airflow Webserver
  # =========================================================================
  airflow-webserver:
    build:
      context: ..
      dockerfile: infrastructure/Dockerfile.airflow
    container_name: airflow-webserver
    command: webserver
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
      AIRFLOW_HOME: /opt/airflow
    env_file:
      - .env.production
    ports:
      - "8081:8080"
    volumes:
      - ./pipelines/airflow/dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./data_lake:/opt/airflow/data_lake
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - esg_network
    restart: unless-stopped


  # =========================================================================
  # ORCHESTRATION: Airflow Scheduler
  # =========================================================================
  airflow-scheduler:
    build:
      context: ..
      dockerfile: infrastructure/Dockerfile.airflow
    container_name: airflow-scheduler
    command: scheduler
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
      AIRFLOW_HOME: /opt/airflow
    env_file:
      - .env.production
    volumes:
      - ./pipelines/airflow/dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./data_lake:/opt/airflow/data_lake
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname $$(hostname)"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - esg_network
    restart: unless-stopped


  # =========================================================================
  # APPLICATION: MCP Server (FastAPI ESG API)
  # =========================================================================
  mcp-server:
    image: python:3.11-slim
    container_name: mcp-server
    command: python -m http.server 8000
    working_dir: /app
    environment:
      REDIS_HOST: redis
      REDIS_PORT: 6379
    ports:
      - "8000:8000"
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/"]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 10s
    networks:
      - esg_network
    restart: unless-stopped


  # =========================================================================
  # TUNNEL: ngrok (Public HTTPS Endpoint)
  # =========================================================================
  ngrok:
    image: ngrok/ngrok:latest
    container_name: ngrok
    command: http mcp-server:8000 --log stdout
    environment:
      NGROK_AUTHTOKEN: ${NGROK_AUTH_TOKEN}
    ports:
      - "4040:4040"
    depends_on:
      mcp-server:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4040/api/tunnels"]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 10s
    networks:
      - esg_network
    restart: unless-stopped


# ============================================================================
# Networks
# ============================================================================
networks:
  esg_network:
    driver: bridge
    name: esg_network


# ============================================================================
# Volumes (for persistent data)
# ============================================================================
volumes:
  postgres_data:
    name: postgres_data
  redis_data:
    name: redis_data
  minio_data:
    name: minio_data
  iceberg_warehouse:
    name: iceberg_warehouse
